# Parallel Programming in Practice

Quando creiamo un programma parallelo dobbiamo pensare a come strutturarlo, il processo si divide in 3 fasi fondamentali:  

1. **Identificare**: si guarda il problema e si analizzano le parti che possono essere farre in parallelo senza rompere la logica.  
2. **Partizionare**: Si prende l'intero workload e lo si divide in pezzi più piccoli.    
3. **Gestire**: Una volta che abbiamo i pezzi di lavoro che gireranno insieme dobbiamo gestire come questi accedono ai dati (evitare race condition), come comunicano tra di loro e come si sincronizzano.  

L'obiettivo è ottenere *SpeedUp* 

$$
\text{Speedup} = \frac{\text{t con 1 processore}}{t con P processori}
$$


## Workflow:  

L'immagine mostra la mappa di workflow che dobbiamo seguire, con i 4 passaggi fondamentali

![parallel Workflow](../../images/parallel_workflow.png)  


1. **Decomposition:**        
    Si parte dal probelma da risolvere 'intero' e lo si spezza in subproblems o task (mattoncini grigi)  
    es: devo dipingere una stanza $\rightarrow$ dipingi parete Nord, dipingi parete Sud, ...

<br>

2. **Assignemnt:**       
    Ho i task e devo decidere chi li fa! Creo i parallel threads o _workers_ e assegno i task ai thread.  
    es: th0 fa parete Nord, th1 fa parete Sud, ... 

<br>

3. **Orchestration:**  
    I thread non sono 'isole' e devono poter comunicare tra di loro, in questa fase definiamo la comunicazione e sincronizzazione.  
    es. th0 non inizare il soffitto finchè th3 non ha finito la parete Ovest  

<br>

4. **Mapping:**    
    Consiste nel passaggio che lega il software al hardware, il SO (o il programmatore) decide quale th fare girare su quale core fisico   


Questi passaggi possono essere fatti dal compilatore, dal hardware o dal programmatore. In HPC per massimizzare le performance è spesso il prgrammatore a dover fare tutto!  


<br> 

# 1. Decomposition

Ci concentriamo sul primo passo: rompere il problema in sottoproblemi.  
Seguiamo la seguente **regola**: creare abbastanza task da tenere occupati tutti i processori (legge di amdahl!)   
La sfida è **identificare le dipendenze**: 
- Non possiamo decomporre a caso! se il task B ha bisogno del risultato del task A non possono girare in parallelo 
- Il segreto sta nel cercare e trovare _mancanza_ di dipendenze!  

<center>

ES: MPEG decoder 

</center>


![MPEG decoder](../../images/MPEG_decoder.png)


Il diagramma mostra un caso reale, un decoder di video. I dati (bit) entrano dall'alto e scendono attraverso vari stadi di elaborazione.   
Abbiamo vari blocchi come VLD, ZigZag/IQuantization/IDCT, Motion Compensation, ... e le frecce indicano dipendenza (non possiamo fare IDCT se non abbiamo prima finito IQuantization)   

Dobbiamo capire come sfruttare il parallelismo in questo caso reale!  

Abbiamo 3 strategie principali per la decomposizione:    

1. Task decomposition (Parallelismo funzionale): guardiamo le funzioni diverse  
    Nel diagramma i task diversi sono quelli in giallo, e di solito, come in questo caso, non sono tantissimi, inoltre se un task è molto più lento di un altro è difficile bilanciare bene il lavoro.   

2. Data decomposition (Parallelismo sui dati): il re dell'HPC  
    Invece di cercare funzioni diverse, prendo la stessa funzione e la applico a pezzi di dati diversi!  
    Nel blocco Motion Compensation notiamo che ci sono fogli sovrapposti, significa che se dobbiamo elaborare 100 macroblocchi possiamo lanciare 100 thread che fanno la **stessa** cosa ma su macroblocchi diversi.  
    È un meccanismo che scala molto bene 

3. Pipeline Decomposition: Paradigma Producer-Consumer
    L'idea è quella di fare pipelining delle task come in una fabbrica.  
    Th0 fa ZigZag sul frame1 e appena finisce passa il frame1 al th1 che fa IQuantization su tale frame, a questo punto il th0 passa subito a fare ZigZag al frame2. In questo modo ZigZag non deve processare e consumare l'intero input, lo manda per pezzi per fare lavorare anche le altre task.   
    Segue un paradigma **Producer-Consumer**, ogni stadio produce dati per quello dopo.  
    Es: Unix pipes `cat file | grep parola | wc` $\rightarrow$ 3 programmi che girano insieme passandosi i dati.  


La decomposizione è nel 99% dei casi responsabilità del programmatore, il compilatore ci prova ma fallisce spesso(in quanto non può fare assunzioni di nessun tipo e rimane sequenziale se davanti a codice con puntatori o dipendenze di dati).  


<br>

## Come decomporre: da dove iniziare   

Function calls: Se abbiamo funzioneA e funzioneB che non si parlano $\rightarrow$ farle girare su th diversi   
Loop iterations: Se abbiamo un ciclo, ogni iterazione è un potenziale task  
**Regola d'oro:** Conviene sempre iniziare spezzettando tanto all'inizio e poi raggruppare i pezzi se notiamo essere lento.   


### **Regole per Task Decomposition:**  
- Flessibilità: I task decomposti non devono essere legati all'architettura, devono essere parametrizzati (se scrivo codice che crea esattamente 4 th e lo lancio su un PC a 16 core allora sto sprecado 12 core!).  
    Il codice deve creare N task, dove N è un parametro che decido quando lancio il programma.  

- Efficienza: Il task decomposto deve fare abbastaza lavoro utile da ripagare il costo di creazione e devono essere il più indipendenti possibile, se ho troppi task che si devono sincronizzare per funzionare allora hai creato un collo di bottiglia per le performance.  

- Semplicità: Il codice deve essere leggibile e si preferisce un codice leggermente meno efficiente ma leggibile rispetto che a uno estremamente complesso e complicato, fare debug su sistemi paralleli è un incubo (es: race condition che si presenta 1 volta su 1000)


### **Regole per Data Decomposition**  

Il legame Task-dati spesso sono facce della stessa medaglia, a volte conviene partire da data decomposition, in particolare se il problema è:
- Data centric: il cuore del programma è manipolare una struttura dati enorme, un immagine, un database, ... 
- Composto da operazioni simili: devo fare la stessa operazione su tutti i pezzi (schiarire un pixel)  

Anche per la data decomposition valgono le regole di qualità: 
- Flessibilità: dimensione dei pezzi non deve essere fissa ma parametrica
- Efficienza: I pezzi dei dati devono essere abbastanza grossi da giustificare il lavoro e devono minimizzare lo scambio di dati tra processori (e massimizzare la località dei dati)
- Semplicità: gestire strutture dati in parallelo è difficile, se si possono usare array semplici allora usali!  

### **Pipeline Decomposition**

Il dato in questo caso non è un oggetto statico che sta fermo e che bisogna consumare per intero, in questo caso il dato è un **FLUSSO** che scorre attraverso vari stadi!  

Il parallelismo in questo caso non si concentra nel dividere i dati e nemmeno nel dividere le funzioni.  
Dividiamo l'intero workflow in **stadi** e assegniamo ogni stadio a un processore diverso!  

**ES:** Unix Pipe `|`

`cat foobar.c | grep bar | wc`

cat legge il file e mentre lo legge inizia a sparare testo, grep riceve testo e filtra le righe con 'bar' sparandole fuori, wc conta le righe che entrano!  
Questi 3 programmi girano contemporanemante, grep non aspetta che cat abbia finito tutto il file, processa le righe man mano che arrivano.  

Questo modo di parallelizzare è ottimo per stream continui, ma la velocità è limitata dallo stadio più lento (collo di bottiglia della catena).  

<br>

---

<br>

<center>

Esempio chiave: 2D grid solver  

</center>

![grid solver](../../images/grid_solver.png)

Abbiamo una grglia di punti $(N+2 \times N+2)$ e vogliamo per ogni punto calcolare la media dei suoi 4 vicini.  

```c++
prev = A[i,j]
A[i,j] = 0.2f * (A[i,j]+A[i,j−1]+A[i−1,j]+A[i,j+1]+A[i+1,j])
diff += fabs(A[i,j]-prev);

if ((diff /(n*n)) < TOLERANCE)
    done = true;
```  

Continuiamo ad aggiornare i punti passando su tutta la grigilia fino a raggiungere la convergenza.   

**Dependency Analysis:**   
Per sapere se possiamo parallelizzare due operazioni usiamo la **Condizione di Bernstein**:   

- IN(R- read set): quali celle di memoria legge la task (input)
- OUT(W - write set): quali celle di memoria scrive (output)

Diremo che due task $T_1$ e $T_2$ sono paralleli se e solo se:
1. L'input di $T_1$ non è parte dell'output di $T_2$ (non leggo ciò che tu stai scrivendo)
2. L'input di $T_2$ non è parte dell'output di $T_1$ (tu non leggi quello che sto scrivendo)
3. L'output di $T_1$ e $T_2$ non si sovrappongono (non scrivono sulla stessa casella)  

Tutte queste 3 regole devono essere contemporaneamente vere, se solo una non lo è allora 
non si può parallelizare (senza usare sincronizzazione).   


Applichiamo Bernstein al nostro algoritmo, vogliamo vedere se possiamo calcolare in parallelo due punti vicini:  
- task1 calcola il punto `A[i,j]`   
    Input: 4 punti adiacenti (tra cui `A[i,j+1]`)
    Out: `A[i,j]`
- task2 calcola il punto `A[i,j+1]`  
    Input: 4 punti adiacenti (tra cui `A[i,j]`)
    Out: `A[i,j+1]`  

Conflitto: task1 scrive `A[i,j]` e task 2 legge `A[i,j]`! stiamo violando al seconda regola di Bernstein, si tratta di una dipendenza Read after Write (RAW).   
Il codice così com'è **non** è parallelizzabile, ogni punto dipende da quello prima.  


**Soluzione: Cambio di Algoritmo**  

Conoscendo l'obiettivo dell'algoritmo iniziale possiamo individuare algorimti che portino allo stesso risultato e che siano parallelizzabili.  
In questo specifico esempio possiamo usare l'algoritmo Red-Black Coloring (quello che si vede nell'immagine in alto).  

Alterniamo celle rosse e nere, e facendo così notiamo che la formula di aggiornamento di una cella dipende solo da celle di colore diverso, non abbiamo dipendenze tra i dati e le condizioni di Bernstein sono soddisfatte.  

Nuovo algoritmo parallelo:
1. Aggiorniamo tutte le celle rosse contemporaneamente
2. Sincornizzazione (Barriera): aspettiamo che tutti abbiano finito le celle rosse
3. Aggiorniamo tutte le celle nere contemporaneamente (usando i valori delle rosse che abbiamo precedentemente calcolato).  


Nota: Il trucco applicato in questo caso è lecito in quanto il nuovo algoritmo porta alla stessa soluzione finale dell'algoritmo sequenziale, questo ci dice che in HPC è neccessario avere una profonda conoscenza del problema che abbiamo davanti per poterlo ottimizzare.  


<br>

# 2. Assignment

Siamo al passo successivo, abbiamo scomposto il problema e ora dobbiamo assegnare questi task ai thread worker.   

### Obiettivi dell'assignment 

1. Workload balance (Bilanciamento): Vogliamo che i thread workers finiscano tutti insieme, se uno lavora troppo rispetto algi altri abbiamo uno spreco di risorse e tempo.  
2. Reduce Comunication (Locality): Vogliamo dare a un thread compiti che usino dati vicini, per minimizzare lo spostamento di dati che è lento.  


Esistono due modi di fare assignment:
1. **Static**: Decido a priori come suddividere il lavoro
2. **Dynamic**: Decido mentre il programma gira

Tornando all'esempio, proviamo due possibili assignment.  

![Blocked vs interleaved](../../images/blocked_vs_interleaved.png)

- Blocked:  
    Tagliamo la matrice a fette orizzontali e assegniamo 3 righe a processore  
    Pro: Ottima località spaziale, P1 accede alle righe 0,1,2 che sono contigue in memoria.  
    Pro2: Poca comunicazione, P1 deve scambiare dati solo con P2 (e P2 con P3, ...)  

- Interleaved:  
    Assegniamo le righe a giro (round robin), quindi riga0 a P0, riga1 a P1, ...  
    Pro: se il lavoro per riga fosse variabile (es: righe in basso più difficili) questo metodo spalmerebbe il carico meglio
    Contro: pessima località! comunicazione esplosiva in quanto ogni riga confina con una riga di un altro processore, ogni processore deve parlare con 2 processori per fare la sua task!  

Nell'esempio la scelta ottimale è senza dubbio blocked, in quanto ha un ottima località e minor comunicazione.     


### Static Assignment  

È quello che abbiamo visto nell'esempio sopra, il programmaotre scrive una formula diretta nel codice che suddivide il carico di lavoro.   
Ha un costo di overhead pari a zero in quanto non abbiamo bisogno di un 'manager' che distribuisca il carico, ogni thread sa già cosa fare appena viene istanziato.  

Usiamo l'assegnazione statica quando il lavoro è **prevedibile**, nell'esempio sopra abbiamo una matrice di 12 righe, quindi 12 task (che sono computazionalmente identici) e abbiamo 4 processori $\rightarrow$ $\frac{12}{4} = 3$ task a thread.  
Risulta perfettamente bilanciato e senza sprechi.  

_Nota_: Usiamo l'assignment statico anche quando i task sono diversi ma il carico totale è conosciuto! se sapessimo a priori che alcuni task sono lunghi e altri corti potremmo dare a th0 un task lungo e uno piccolo, mentre algi altri th diamo task medi. Se riusciamo a fare si che la somma del lavoro che devono fare i th sia la stessa, abbiamo vinto.  

**Conclusione:** L'assegnazione stastica è la migliore se conosciamo il carico di lavoro in anticipo.  

