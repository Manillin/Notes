# Parallel Programming in Practice

Quando creiamo un programma parallelo dobbiamo pensare a come strutturarlo, il processo si divide in 3 fasi fondamentali:  

1. **Identificare**: si guarda il problema e si analizzano le parti che possono essere farre in parallelo senza rompere la logica.  
2. **Partizionare**: Si prende l'intero workload e lo si divide in pezzi più piccoli.    
3. **Gestire**: Una volta che abbiamo i pezzi di lavoro che gireranno insieme dobbiamo gestire come questi accedono ai dati (evitare race condition), come comunicano tra di loro e come si sincronizzano.  

L'obiettivo è ottenere *SpeedUp* 

$$
\text{Speedup} = \frac{\text{t con 1 processore}}{t con P processori}
$$


## Workflow:  

L'immagine mostra la mappa di workflow che dobbiamo seguire, con i 4 passaggi fondamentali

![parallel Workflow](../../images/parallel_workflow.png)  


1. **Decomposition:**        
    Si parte dal probelma da risolvere 'intero' e lo si spezza in subproblems o task (mattoncini grigi)  
    es: devo dipingere una stanza $\rightarrow$ dipingi parete Nord, dipingi parete Sud, ...

<br>

2. **Assignemnt:**       
    Ho i task e devo decidere chi li fa! Creo i parallel threads o _workers_ e assegno i task ai thread.  
    es: th0 fa parete Nord, th1 fa parete Sud, ... 

<br>

3. **Orchestration:**  
    I thread non sono 'isole' e devono poter comunicare tra di loro, in questa fase definiamo la comunicazione e sincronizzazione.  
    es. th0 non inizare il soffitto finchè th3 non ha finito la parete Ovest  

<br>

4. **Mapping:**    
    Consiste nel passaggio che lega il software al hardware, il SO (o il programmatore) decide quale th fare girare su quale core fisico   


Questi passaggi possono essere fatti dal compilatore, dal hardware o dal programmatore. In HPC per massimizzare le performance è spesso il prgrammatore a dover fare tutto!  


<br> 

### 1. Decomposition

Ci concentriamo sul primo passo: rompere il problema in sottoproblemi.  
Seguiamo la seguente **regola**: creare abbastanza task da tenere occupati tutti i processori (legge di amdahl!)   
La sfida è **identificare le dipendenze**: 
- Non possiamo decomporre a caso! se il task B ha bisogno del risultato del task A non possono girare in parallelo 
- Il segreto sta nel cercare e trovare _mancanza_ di dipendenze!  

<center>

ES: MPEG decoder 

</center>


![MPEG decoder](../../images/MPEG_decoder.png)


Il diagramma mostra un caso reale, un decoder di video. I dati (bit) entrano dall'alto e scendono attraverso vari stadi di elaborazione.   
Abbiamo vari blocchi come VLD, ZigZag/IQuantization/IDCT, Motion Compensation, ... e le frecce indicano dipendenza (non possiamo fare IDCT se non abbiamo prima finito IQuantization)   

Dobbiamo capire come sfruttare il parallelismo in questo caso reale!  

Abbiamo 3 strategie principali per la decomposizione:    

1. Task decomposition (Parallelismo funzionale): guardiamo le funzioni diverse  
    Nel diagramma i task diversi sono quelli in giallo, e di solito, come in questo caso, non sono tantissimi, inoltre se un task è molto più lento di un altro è difficile bilanciare bene il lavoro.   

2. Data decomposition (Parallelismo sui dati): il re dell'HPC  
    Invece di cercare funzioni diverse, prendo la stessa funzione e la applico a pezzi di dati diversi!  
    Nel blocco Motion Compensation notiamo che ci sono fogli sovrapposti, significa che se dobbiamo elaborare 100 macroblocchi possiamo lanciare 100 thread che fanno la **stessa** cosa ma su macroblocchi diversi.  
    È un meccanismo che scala molto bene 

3. Pipeline Decomposition: Paradigma Producer-Consumer
    L'idea è quella di fare pipelining delle task come in una fabbrica.  
    Th0 fa ZigZag sul frame1 e appena finisce passa il frame1 al th1 che fa IQuantization su tale frame, a questo punto il th0 passa subito a fare ZigZag al frame2. In questo modo ZigZag non deve processare e consumare l'intero input, lo manda per pezzi per fare lavorare anche le altre task.   
    Segue un paradigma **Producer-Consumer**, ogni stadio produce dati per quello dopo.  
    Es: Unix pipes `cat file | grep parola | wc` $\rightarrow$ 3 programmi che girano insieme passandosi i dati.  


La decomposizione è nel 99% dei casi responsabilità del programmatore, il compilatore ci prova ma fallisce spesso(in quanto non può fare assunzioni di nessun tipo e rimane sequenziale se davanti a codice con puntatori o dipendenze di dati).  


<br>

### Come decomporre: da dove iniziare   

Function calls: Se abbiamo funzioneA e funzioneB che non si parlano $\rightarrow$ farle girare su th diversi   
Loop iterations: Se abbiamo un ciclo, ogni iterazione è un potenziale task  
**Regola d'oro:** Conviene sempre iniziare spezzettando tanto all'inizio e poi raggruppare i pezzi se notiamo essere lento.   


**Regole per Task Decomposition:**  
- Flessibilità: I task decomposti non devono essere legati all'architettura, devono essere parametrizzati (se scrivo codice che crea esattamente 4 th e lo lancio su un PC a 16 core allora sto sprecado 12 core!).  
    Il codice deve creare N task, dove N è un parametro che decido quando lancio il programma.  

- Efficienza: Il task decomposto deve fare abbastaza lavoro utile da ripagare il costo di creazione e devono essere il più indipendenti possibile, se ho troppi task che si devono sincronizzare per funzionare allora hai creato un collo di bottiglia per le performance.  

- Semplicità: Il codice deve essere leggibile e si preferisce un codice leggermente meno efficiente ma leggibile rispetto che a uno estremamente complesso e complicato, fare debug su sistemi paralleli è un incubo (es: race condition che si presenta 1 volta su 1000)


**Regole per Data Decomposition**  

Il legame Task-dati spesso sono facce della stessa medaglia, a volte conviene partire da data decomposition, in particolare se il problema è:
- Data centric: il cuore del programma è manipolare una struttura dati enorme, un immagine, un database, ... 
- Composto da operazioni simili: devo fare la stessa operazione su tutti i pezzi (schiarire un pixel)  

Anche per la data decomposition valgono le regole di qualità: 
- Flessibilità: dimensione dei pezzi non deve essere fissa ma parametrica
- Efficienza: I pezzi dei dati devono essere abbastanza grossi da giustificare il lavoro e devono minimizzare lo scambio di dati tra processori (e massimizzare la località dei dati)
- Semplicità: gestire strutture dati in parallelo è difficile, se si possono usare array semplici allora usali!  

**Pipeline Decomposition**

Il dato in questo caso non è un oggetto statico che sta fermo e che bisogna consumare per intero, in questo caso il dato è un **FLUSSO** che scorre attraverso vari stadi!  

Il parallelismo in questo caso non si concentra nel dividere i dati e nemmeno nel dividere le funzioni.  
Dividiamo l'intero workflow in **stadi** e assegniamo ogni stadio a un processore diverso!  

<center>

ES: Unix Pipe `|`

</center>

`cat foobar.c | grep bar | wc`

cat legge il file e mentre lo legge inizia a sparare testo, grep riceve testo e filtra le righe con 'bar' sparandole fuori, wc conta le righe che entrano!  
Questi 3 programmi girano contemporanemante, grep non aspetta che cat abbia finito tutto il file, processa le righe man mano che arrivano.  

Questo modo di parallelizzare è ottimo per stream continui, ma la velocità è limitata dallo stadio più lento (collo di bottiglia della catena).  