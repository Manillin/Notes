# Lexer (Analizzatore Lessicale)

### Riconoscimento di token:
Il compito principale del **_LEXER_** è di trasformare la sequenza di caratteri in input in una sequenza di **_TOKEN_**.  
Il lexer agisce come subroutine del parser e, ad ogni chiamata, restituisce a quest'ultimo un _token_.  


### Token:
Un token è un oggetto astratto che rappresenta un elemento significativo per l'analisi sintattica.  
Es: _numeri, identificatori, parole riservate al linguaggio di progr, ..._

**Lessema** $\rightarrow$ una sottostringa dell'input che è istanza concreta di un token.  
Le descrizioni formali (**pattern**) dei lessemi da rionoscere come token sono le _espressioni regolari_.  

### Cos'è un Token:
- Oggetto caratterizzato da due attributi, un _nome_ (obbligatorio) e un _valore_ (opzionale). 
- **token name** $\rightarrow$ identificatore di un token (deve essere lo stesso per lexer e parser)
- Tipici nomi di token sono: **id, number, literal, string, ...**

Es:  
<center>  

`somma = 0;`

</center>  

il lexer restituirà:
- (**id**, `somma`)
- **assignment**
- (**number**, 0)

--- 

### LEX:

L'analizzatore lessicale è l'unico modulo del compilatore che legge il codice sorgente vero e proprio, difatti a volte ci si riferisce al lexer come 'scanner'.  

Lex è un generatore di analizzatori lessicali, si tratta di un sw in grado di generare altri programmi capaci di riconoscere stringhe di un linguaggio regolare.   

Un generico programma Lex contiene le seguenti sezioni:  

`Dichiarazioni`  
%%  
`Regole di traduzione`  
%%   
`Funzioni Ausiliarie`  
\<opt il main()\>

