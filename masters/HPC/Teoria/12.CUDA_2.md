# CUDA Memory Model: 



1. **Risorse locali del SM**: 
    - registers: ogni thread ha i suoi registri privati e velocissimi 

    - shared memory: è una memoria speciale condivisa tra i thread dello stesso blocco, e solo da loro; se il blocco 0 scrive in shared il blocco 1 non potrà vederlo.  
2. **Memorie Globali**: 
    - Global memory: È la RAM della GPU, grande ma lenta; tutti i thread di tutti i blocchi possono leggerla e scriverci sopra 
    - Constant memory: memoria di sola lettura, ottimizzata per dati che non cambiano 
    - Texture memory: memoria di sola lettura, ottimizzata per grafica (cache speciale 2D)

<center>

![memory model](../../images/mem_model.png)

</center>


### Global Memory:  

È la RAM ed è la memoria più grande sulla GPU (in termini di GB), visibile da tutti e i dati sopravvivono tra un kernel e l'altro.  
Prestazioni: 
- **High Latency**: Latenza altissima (centinaia di cicli), ogni volta che bisogna leggere da qui il thread si blocca per molto tempo.   
- **High Bandwidth**: Banda alta (passano tanti dati) MA solo se usata bene!  


Possiamo creare dati in global memory in 2 modi:
1. **Allocazione statica**:  `__device__ int x;`
    Usiamo il qualificatore `__device__`; dichiarata fuori dalle funzioni, l'analogo di una variabile globale in C.  

2. **Allocazione dinamica**: si usano i puntatori  
    Si dichiara un puntatore al tipo di dato `int* ptr` e si chiama dal host `cudaMalloc`; per liberare spazio si usa `cudaFree`.  



### Accesso alla Global Memory:  

Il modo di accedere alla global memory impatta la **performance**.  
Esistono due tipi di accesso:
- **offset access**: I thread leggono dati consecutivi ma spostati in avanti.  
    - avviene quello che si definisce come **coalescence**: la GPU vede che tutti i thread chiedono dati vicini, fa un unica richiesta grossa alla memoria (es. pacchetto da 128Byte); massima efficienza.  

- **stride access**: i thread leggono dati distanziati (es: th0 legge idx0, th1 legge idx2, th2 legge idx4, ...)  
    - qui nasce un **problema**: i dati richiesti non sono vicini (soprattutto per stride larghi), non stanno nello stesso pacchettoda 128Byte; la GPU è costretta a fare tante piccole richieste separate, la banda crolla.  


<center>

![stride vs offset](../../images/stride_vs_offset.png)

</center>


**Conclusione**: Quando si progettano e si usano strutture dati (es. matrici), bisogna fare il possibile per garantire che thread adiacenti (tid, tid+1, ...) leggano dati adiacenti in memoria!  



<br>

## Cache Model:  


Dall'architettura Fermi in poi NVIDIA ha introdotto una gerarchia di cache dentro le GPU.  

- `L2 Cache`: È memoria condivisa tra **tutti** gli SM  
    - Capienza grande (in ordine di MB)
    - Tutti gli accessi alla global memory passano prima di qui, è il punto di incontro

- `L1 Cache`: privata per ogni singolo SM
    - È una memoria molto veloce ma piccola  
    - **Conflitto L1/Shared-Memory**: Fisicamente la cache L1 e la shared memory usano lo stesso baco di memoria on chip (spesso 64kb totali)  
        - se si vuole tanta shared memory, allora rimane poca L1
        - se non si usa shared memory in un programma si può dare l'intero spazio alla L1 
        - è una **partizione logica dinamica**: Una partizione del banco di memoria condiviso viene assegnata alla L1 cache e il resto è allocata per la shared memory; la shared memory è fisicamente la stessa per ogni SM ma diversa per ogni blocco dentro lo SM, una porzione della shared memory è data ad ogni blocco ed è l'hardware ad aggiungere automaricamente l'offset per gli accessi a seconda del blocco; in questo modo fisicamente usano lo stesso banco di memoria ma logicamente lavorano su spazi di memoria separati e protetti.  
    - Possiamo configurare la L1 e la shared memory per decidere la partizione di interese, per farlo usiamo `cudaFuncSetCacheConfig`.  

Si possono anche avere **Load Policies**, che permettono di decidere se usare o non usare la cache L1 (il vantaggio sta nel scegliere la policy giusta al momento giusto, solitamente si salta il caching in L1 quando dobbiamo leggere un dato una sola volta, evitando di inquinare la L1 buttando dati che magari potevano servire).   



<br>

### Meccanica delle operazione di `Load` sulla global memory:

La load avviene attraverso 3 passaggi: 

1. **Richiesta**: tutti i 32 thread del warp presentano il loro indirizzo di memoria
2. **Analisi** (coalescing): l'unità hardware guarda i 32 indirizzi per vedere se sono tutti vicini (es. da 0 a 127) o se sono sparpagliati
3. **Segmentazione**: L'unità calcola quante transazioni (viaggi verso la memoria) servono
    - se gli indirizzi cadono tutti in un unico blocco da 128B $\rightarrow$ 1 transazione
    - se cadono in $N$ blocchi diversi $\rightarrow$ $N$ transazioni (lento)  

ES: Abbiamo le seguenti istruzioni con la load  
```cpp
int i = threadIdx.x;
float x = A[i];
```

L'istruzione è unica ma viene eseguita (ovviamente) come SIMT, quindi:
- th0 calcola i=0 e il suo indirizzo target è `&A[0]`  
- th1 calcola i=1 e il suo indirizzo target è `&A[1]`
- th_n calcola i=n e il suo indirizzo target è `&A[n]` 

Quando il warp arriva all'istruzione di load, la load/store unit vede arrivare 32 indirizi diversi contemporaneamente! vogliamo che i dati siano contingui per fare il minor numero di transazioni possibili.  

La segmentazione ci spiega perchè gli accessi in stride fanno crollare la banda, servono più transazioni!  


<br>

### Allineamento: 

**Le GPU leggono blocchi da 128B**, per minimizzare le transazioni verso la memoria, gli indirizzi dovrebbero essere allineati (l'indirizzo di partenza deve esere un multiplo di 32 o 128), ma questo non è sempre il caso.   

**FONDAMENTALE**: La memoria della GPU è come un foglio a quadretti predisegnato e diviso in blocchi fissi da 128Byte.  
NON puoi chiedere alla GPU di leggere "128Byte partendo dall'indirizzo 50", la GPU può leggere SOLAMENTE blocchi di memoria che iniziano a indirizzi multipli di 128 (0, 128, 256, ...).  
I blocchi sono indirizzi fisici nella RAM allineati rigidamente.  


prendiamo una matrice da 100 colonne (`width=100`) float, quindi 400 Byte.    
il warp è composto da 32 thread, e ogni thread legge un float (4 Byte), quindi un warp collettivamente vuole leggere 32x4 = 128Byte!  

Ricordiamo che gli accessi in memoria sono a blocchi fissi da 128B, per leggere la prima riga da 100 elementi devo leggere 400 byte, la GPU lancia i warp:
- b0 [0 - 127] : w0 chiede questo blocco, è contiguo e quindi eseguito in una singola transazione
- b1 [128 - 255]: w1 chiede questo blocco, uguale a sopra.

Per questa prima riga ogni richiesta del warp si traduce in una singola transazione in memoria, il che è ottimo.  

Il problema nasce quando vogliamo leggere la riga1, ossia i primi 128B a partire dal byte 400.  
Per leggere i 32 float della prima riga il warp responsabile ha bisogno degli indirizzi da [400 - 527]!   
Ma l'indirizzo 400 non è multiplo di 128! e siccome i dati sono organizzati in blocchi da 128B avremo queta situazione:
- b3 [384 - 511]
- b4 [512 - 639]  

Il warp che vuole leggere da 400 a 527 dovrà quindi aprire 2 blocchi! il blocco3 e il blocco4, il che si traduce in 2 transazioni!  
Tutti gli altri warp di questa riga dovranno fare 2 transazioni per portarsi in memoria gli elementi che servono, stiamo facendo 2 transazioni per ogni richiesta di warp da quel punto il che penalizza fortemente le performance (in quanto viaggiare in memoria è l'operazione più lenta).  


Per risolvere questo problema usiamo il Padding 

<br>


### Padding:

Dobbiamo allungare la riga della matrice per fare in modo che la successiva inizi a un indirizzo multiplo di 128, per fare in modo di evitare doppia transazione per richiesta di warp $\rightarrow$ aggiungiamo byte vuoti alla fine di ogni riga per garantire queta proprietà!  

- `cudaMalloc`: alloca un blocco 1D semplice, non garantisce l'allineamento delle righe interne se si usa una logica 2D
- `cudaMallocPitch`: è la soluzione intelligente! bisogna fornire il numero di righe e di colonne e lui in automatico calcola quanto padding aggiungere.  
    Restituisce un valore chiamato `pitch`


Esempio con `pitch`

```cpp
int width = 64, int height = 64;
float a[width][height];
float* devPtr;
int pitch;

cudaMallocPitch(&devPtr, &pitch, width * sizeof(float), height);
cudaMemCopy2D(devPtr, pitch, a, width * sizeof(float), heigth, cudaMemcpyHostToDevice);

// Kernel call 
```


### CUDA Shared Memory:  

È memoria che risiede dentro l'SM, logicamente privata per ogni blocco dello SM.  
- **bassa latenza**: 1-2 cicli di clock, veloce quasi quanto i registri  
- **volatile**: quando il blocco finisce l'esecuzione la sua shared memory viene cancellata
- **configurabile**: condivide lo stesso spazio fisico della L1 cache, possiamo decidere quanta memoria allocare alla shared.  

Si usa come una cache gestita dal programmatore, solitamente segue questo flusso di uso:
1. load: i thread collaborano per copiare i dati dalla global memory alla shared memory 
2. synchronize: prima di iniziare i calcoli, bisogna assicurarsi che tutti i thread abbiano finito di caricare i dati in memoria (`__synchthreads()`)
3. use: si fanno i calcoli pesanti leggendo e scrivendo direttamente sulla shared memory
4. synchronize: si sincronizza nuovamente per aspettare che tutti i thread abbiano finito
5. write: si copiano i risultati finali dall shared alla global memory  



### Banchi di memoria   
La shared memory non è un blocco unico, è divisa in **32 banchi** verticali.  
Ogni banco può contenere una quantità di word (4Byte) che varia in base alla quantità di memoria che allochiamo alla shared-memory.  

**Modalità di accesso ottimali:**   
- parallelismo: se 32 thread accedono a 32 banchi diversi, l'accesso avviene in un solo ciclo di clock, è la velocità massima alla quale possiamo ambire.  
- multicast/broadcast: se più thread chiedono lo stesso indirizzo, l'hardware lo legge una volta sola e invia il dato a tutti; 1 ciclo di clock 

<br>

**Bank Conflict:**      
Avviene quando due thread vogliono accedere a indirizzi diversi che però sono mappati nello stesso banco. Ogni banco ha una sola porta, non può servire contemporaneamente due thread, di conseguenza l'accesso viene serializzato.  


**Allocazione della Shared Memory**  
Abbiamo due modi per allocare la shared memoy: staticamente / dinamicamente

1. **Allocazione Statica**:  
    La usiamo se sappiamo la dimensione a compile time, è facile e 

```cpp

__global__ void myKernel(...){
    __shared__ int shmem[64];
    ... 
}
```


2. **Allocazione Dinamica**:   
    Da preferire qunando dobbiamo usare un kernel generico che funzioni per una dimensione N che non sappiamo a priori (magari decisa dall'utente nel main).   
    Definiamo la dimensione della memoria tramite una variabile che passiamo al kernel nel momento della sua invocazione.  

```cpp
extern __shared__ int dynshmem[];
__global__ void myKernel(...)
{
    ...
    dynshmem[i] = ... ;
}

void hostFunc(...)
{
    ...
    myKernel<<<<gs, bs, MEMSZ>>>>();
}

```

## Sincronizzazione tra Thread:  

I thread sono indipendenti ma a volte devono collaborare (come nel tiling per caricare i dati), per farlo devono aspettarsi.  


### Sincronizzazione in un blocco: 

**`synchthreads()`** è una barriera fornita dal API di CUDA.  
- ha scope limitato a singolo blocco, questa funzione sincronizza solamente i thread di un blocco (non sincronizza blocchi diversi).  
- aspetta che **tutti** i thread dello stesso blocco la raggiungano, i thread che arrivano per primi rimangono in attesa degli altri
- è inseribile all'interno di branch condizionali ma bisogna prestare particolare attenzione in questi casi, possiamo farlo solo se siamo sicuri che **tutti** i thread entrino nello stesso branch condizionale, altrimenti si crea un deadlock.  

Esempio di utilizzo:

```cpp
__global__ void update_race(int* x, int* y)
{
    int i = threadIdx.x + blockDim.x * blockIdx.x;
    if (i==0) *x = 1;
    __synchthreads();
    if (i==1) *y = *x;
}

// main.cu
update_race<<<1,2>>>(d_x,d_y);
```

se non mettiamo la barriera di sincronizzazione avremo una race condition, il thread1 potrebbe eseguire prima del thread0 in quanto sono indipendenti.  


**thread divergence e deadlocks**:  

La thread divergence è quella che causa i deadlock, avviene quando mettiamo una barriera di sincronizzazione dentro un branch condizionale dentro al quale non entrano tutti i thread.  

```cpp
if (threadIdx.x < 16)
{
    myFunc_then();
    __synchthreads();
}else if{
    myFunc_else();
}
```

In questo esempio la prima metà dei thread entra nell'if e incontrano la barriera, quindi si fermano.  
La seconda metà dei thread entra nell'else e finiscono l'esecuzione.  
I thread che hanno incontrato la barriera rimangono bloccati per sempre in quanto gli altri thread non la raggiungeranno mai, si crea una situazione di stallo.  
Il kernel va in timeout e viene ucciso dal driver!  

### Sincronizzazione tra grid:

CUDA non fornisce una barriera globale che fermi tutti i blocchi della griglia, per questioni di scalabilità hardware.  
Se hai una GPU piccola, non tutti i blocchi stanno dentro gli SM, alcuni saranno in attesa e se il blocco 1 (attivo) deve aspettare il blocco $N$ che è in attesa si creerebbe un deadlock.  

Per sincronizzare una grid si usa il **Kernel Split**:  
Usiamo come barriera globale il kernel launch stesso, invece di usare un kernel unico dividiamo il lavoro in due kernel.  
1. lanci kernel1
2. il kernel1 finisce, esce dalla coda dello stream
3. la memoria globale ora è consistente e aggiornata 
4. lanci il kernel2 (che aveva bisogno del lavoro del kernel1)  


```cpp
update_x<<<<...>>>>(...); 
/*  Barriera implicita tra i due lanci  */
update_y<<<...>>>(...);
```
**Nota importante**:  
Ricordiamo che lanciare un kernel è un'operazione asincrona, la CPU li lancia e poi riprende subito dopo il controllo.  
La GPU dispone di stream di esecuzione, e kernel diversi all'interno dello stesso stream eseguono in modo serializzato, uno dietro l'altro!  
Quindi quello che succede veramente è:

1. CPU lancia kernel1, la CPU inserisce il comando 'esegui kernel1' in una coda (su uno stream) gestito dal driver.  
2. CPU lancia kernel2, e inseriesce il comando 'esegui kernel2' nella stesas coda 
3. La GPU guarda la coda:
    - vede kernel1 in testa, lo esegue (e lo toglie dalla coda)
    - mentre kernel1 esegue, kernel2 rimane in attesa e fermo 
    - quando kernel1 finisce completamente e la memoria è salvata, la GPU torna a guardare la coda e vede kernel2 e lo fa partire 


<br>

### Sincronizzazione con operazioni atomiche:  

Usiamo le funzioni atomiche quando più thread (di blocchi diversi) devono aggiornare un dato comunune.  
**NON** servono a mettere in ordine i thread in termini di tempo ma a **proteggere** l'accesso ai dati.  

Il classico problema è quello della _Race Condition_  

```cpp
__global__ void update(int*x)
{
    x = x + 1:
}
```

Se abbiamo 128 thread tutti proveranno a incrementare la variaible globale x insieme, otteniamo un risultato sbagliato.  

Questo esempio lo risolviamo con la `int atomicAdd(address, val)` 
- blocca l'indirizzo di memoria 
- legge il valore attuale 
- somma val e scrive il risultato in modo atomico 
- sblocca l'indirizzo

_Nota:_ questa operazione restituisce un `int`, in particolare restituisce il valore vecchio che aveva la variabile prima dell'operazione atomica (utile se vogliamo ad esempio fare prendere a ogni thread un 'biglietto' )  

**Funzioni CUDA per operazioni atomiche**:  
- aritmetica: `add`, `sub`, `inc`, `dec`  
- bitwise: `and`, `or`, `xor`
- confronto: `min`, `max`
- scambio: `exch`(swap), `cas` (compare and swap)  

[Atomic functions in CUDA (docs)](https://docs.nvidia.com/cuda/cuda-c-programming-guide/#atomic-functions)

Le operazioni di atomicExch e atomicCas sono i mattoni fondamentali per costruire qualsiasi altra struttura dati in modo thread safe.  


### Performance of atomic operations: 

È un concetto fondamentale quando si usano operazioni atomiche! bisogna essere molto consapevoli del fatto che le operazioni atomiche **NON sono gratis**.  

Se 1000 thread fanno `atomicAdd` sullo stesso indirizzo l'hardware è costretto a serializzare le richieste, trasformiamo la GPU in una lenta macchina sequenziale.  

La **best practice** prevede di creare copie locali al blocco e fare lavorare i thread sulla shared memory per assicurare velocità, solo alla fine mettiamo insieme tutti i pezzi insieme.  

ES: Creazione di un istogramma  
1. Ogni blocco si crea un istrogramma privato in shared memory 
2. I thread del blocco usano operazioni atomiche veloci (in quanto in shared mem) per aggiornare il valore locale 
3. solo alla fine, il blocco aggiorna l'istrogramma globale (in global memory RAM) 

Questo riduce la contesa di un fattore pari alla dimensione del blocco!  


<br>

### Constan Memory  

Riprendiamo il modello della memoria

<center>

![memory model](../../images/mem_model.png)

</center>

La **Constant Memory** è una regione speciale di memoria (limitata a 64KB) che è **ottimizzata** per dati che non cambiano durante l'esecuzione del kernel.    

I dati della constant memory sono fisicamente lontani, è come se fossero in RAM, ma disponiamo di una piccola **constant cache** che è quella veloce che ci permette di ottenere speedup.  

La constant memory è progettata per il caso in cui **tutti** i thread di un warp leggono lo stesso indirizzo contemporaneamente (es. variabili di formule, filtri per immagini).  
È memoria NON modificabile, read-only.  


La Constant Cache è un circuito single-port con una rete di distribuzione a ventaglio; ha 1 input ma può servire lo stesso valore a 32 thread diversi (aka broadcast).  
Usiamo la constant cache per operazioni di tipo broadcast e read only perchè
- risparmiamo banda L1: usare la constant cache per valori costanti di tipo broadcast mi permette di lasciare la L1 e la shared memory libere per caricare dati variabili!  
- attenzione: la constant cache è ottima se tutti leggono in broadcast, ma diventa pessima se tutti i thread leggono indirizzi diversi! per quelle operazioni la L1 o shared memory sono le migliori.    


**Allocazione e uso della constant memory**:  


1. dichiarazione: si dichiara come variabile globale (fuori dal main) con la keyword `__constant__`, ha dimensione fissa statica, non possiamo fare malloc qui
2. per la copia usiamo: `cudaMemcpyToSymbol()` non la cudaMemcpy normale 
3. dentro al kernel la usiamo come una variabile globale, non va passata come argomento del kernel 


```cpp
__constant__ float P; // static 

__global__ void kernel(float *a, int N)
{
    int idx = blockIdx.x * blockDim.x + threadIdx.x;
    if (idx < N){
        a[idx] = a[idx] * P;
    }
}

int main(void)
{
    ... 
    float Euro_dollar = 1.11;
    cudaMemcpyToSymbol(P, &Euro_dollar, sizeof(Euro_dollar), cudaMemcpyHostToDevice);
    ... 
}
```


<br>

### CUDA registers:   

È la memoria più importante per il calcolo ad alte prestazioni.  
Corrispondono alle variabili locali dentro il kernel, è memoria immediata, in assoluto la più veloce (0 cicli) ed è _privata_ per ogni thread.     

**Register Pressure:**  
Ogni SM ha un numero finito di registri, che vengono spartiti tra tutti i blocchi attivi.  

- Più registri usa ogni thread $\rightarrow$ meno thread possono girare contemporaneamente sull'SM
- Se usiamo troppi registri crolla l'Occupancy (numero di warp attivi) e aumenta la latenza di memoria

Bisogna scrivere codice che usa il minor numero di variabili locali inutili per massimizzare le performance.  


Il compilatore `nvcc` cerca di ottimizzare l'uso dei registri, possiamo aiutarlo con direttive speciali come `__launch_bounds__ (maxThreadsPerBlock, MinBlocksPerMultiprocessor)`



### CUDA Local Memory:  

La local memory è uno spazio della global memory (RAM lenta) che viene riservato privatamente per ogni thread.  

Viene usata nel caso del **Register Spilling**:  
- abbiamo register spilling quando usiamo troppe variabili locali e i registri fisici sono finiti. Il compilatore deve 'parcheggiare' le variabil extra in RAM (crolla performance).  

Possiamo compilare il nostro codice con il flag `-ptxas-options=-v` per guardare se la local memory è stata usata.  


penso ultima domanda. ma la constan memory è fisicamente in RAM (cioe vicino alla ram), e poi ho una cache di constant memory? cioè ho due componenti, quella grande in ram che contiene i dati e poi la cache che mi fa avere il boost di performance.  



